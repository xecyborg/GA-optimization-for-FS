{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deap\n",
        "!pip install scoop"
      ],
      "metadata": {
        "id": "GAwuNaACu0Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJiBwusQvBCi"
      },
      "outputs": [],
      "source": [
        "import collections.abc\n",
        "collections.Iterable = collections.abc.Iterable\n",
        "collections.Mapping = collections.abc.Mapping\n",
        "collections.MutableSet = collections.abc.MutableSet\n",
        "collections.MutableMapping = collections.abc.MutableMapping\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from deap import creator, base, tools, algorithms\n",
        "from scoop import futures\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy import interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import models, layers, optimizers\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfData = pd.read_csv('/content/drive/MyDrive/GA/bank.csv', sep=';')"
      ],
      "metadata": {
        "id": "G4NqhiT8vYFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the classification labels to numbers\n",
        "# Get classes and one hot encoded feature vectors\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(dfData['y'])\n",
        "allClasses = le.transform(dfData['y'])\n",
        "allFeatures = dfData.drop(['y'], axis=1)\n",
        "\n",
        "# Form training, test, and validation sets\n",
        "X_trainAndTest, X_validation, y_trainAndTest, y_validation = train_test_split(allFeatures, allClasses, test_size=0.20, random_state=40)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_trainAndTest, y_trainAndTest, test_size=0.20, random_state=40)"
      ],
      "metadata": {
        "id": "wpG7pC5DvYCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getFitness(individual, X_train, X_test, y_train, y_test, user_batch = 1024):\n",
        "    cols = [index for index in range(len(individual)) if individual[index] == 0]\n",
        "    X_train_parsed = X_train.drop(X_train.columns[cols], axis=1)\n",
        "    X_train_oh_features = pd.get_dummies(X_train_parsed)\n",
        "    X_test_parsed = X_test.drop(X_test.columns[cols], axis=1)\n",
        "    X_test_oh_features = pd.get_dummies(X_test_parsed)\n",
        "\n",
        "    shared_features = set(X_train_oh_features.columns) & set(X_test_oh_features.columns)\n",
        "    remove_from_train = set(X_train_oh_features.columns) - shared_features\n",
        "    remove_from_test = set(X_test_oh_features.columns) - shared_features\n",
        "    X_train_oh_features = X_train_oh_features.drop(list(remove_from_train), axis=1)\n",
        "    X_test_oh_features = X_test_oh_features.drop(list(remove_from_test), axis=1)\n",
        "\n",
        "    num_cols = X_train_parsed.select_dtypes(include=np.number).columns\n",
        "    scaler = StandardScaler()\n",
        "    X_train_num_features = scaler.fit_transform(X_train_parsed[num_cols])\n",
        "    X_test_num_features = scaler.transform(X_test_parsed[num_cols])\n",
        "\n",
        "    X_train_features = np.hstack((X_train_oh_features, X_train_num_features))\n",
        "    X_test_features = np.hstack((X_test_oh_features, X_test_num_features))\n",
        "\n",
        "    # Build a deep neural network model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(20, activation='relu', input_shape=(X_train_features.shape[1],)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model and fit it on the training data\n",
        "    optimizer = optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_features, y_train, epochs=1, batch_size=user_batch, verbose=0)\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test_features, y_test, verbose=0)\n",
        "\n",
        "    # Return calculated accuracy as fitness\n",
        "    return (accuracy,)"
      ],
      "metadata": {
        "id": "62aITiCMvX_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Individual\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Create Toolbox\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, len(dfData.columns) - 1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Continue filling toolbox...\n",
        "toolbox.register(\"evaluate\", getFitness, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
        "toolbox.register(\"mate\", tools.cxOnePoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
      ],
      "metadata": {
        "id": "I1iGiLpsvX71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getHof():\n",
        "\n",
        "    # Initialize variables to use eaSimple\n",
        "    numPop = 100\n",
        "    numGen = 10\n",
        "    pop = toolbox.population(n=numPop)\n",
        "    hof = tools.HallOfFame(numPop * numGen)\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"avg\", np.mean)\n",
        "\n",
        "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=numGen, stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "    return hof"
      ],
      "metadata": {
        "id": "7jZrhJ_lvX54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getMetrics(hof):\n",
        "\n",
        "    # Get list of percentiles in the hall of fame\n",
        "    percentileList = [i / (len(hof) - 1) for i in range(len(hof))]\n",
        "\n",
        "    # Gather fitness data from each percentile\n",
        "    testAccuracyList = []\n",
        "    validationAccuracyList = []\n",
        "    individualList = []\n",
        "    for individual in hof:\n",
        "        testAccuracy = individual.fitness.values\n",
        "        validationAccuracy = getFitness(individual, X_trainAndTest, X_validation, y_trainAndTest, y_validation)\n",
        "        testAccuracyList.append(testAccuracy[0])\n",
        "        validationAccuracyList.append(validationAccuracy[0])\n",
        "        individualList.append(individual)\n",
        "    testAccuracyList.reverse()\n",
        "    validationAccuracyList.reverse()\n",
        "    return testAccuracyList, validationAccuracyList, individualList, percentileList"
      ],
      "metadata": {
        "id": "h6zKMGfCvX2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    individual = [1 for i in range(len(allFeatures.columns))]\n",
        "    testAccuracy = getFitness(individual, X_train, X_test, y_train, y_test)\n",
        "    validationAccuracy = getFitness(individual, X_trainAndTest, X_validation, y_trainAndTest, y_validation)\n",
        "    print('\\nTest accuracy with all features: \\t' + str(testAccuracy[0]))\n",
        "    print('Validation accuracy with all features: \\t' + str(validationAccuracy[0]) + '\\n')\n",
        "\n",
        "    start = time.time()\n",
        "    hof = getHof()\n",
        "    testAccuracyList, validationAccuracyList, individualList, percentileList = getMetrics(hof)\n",
        "\n",
        "    # Get a list of subsets that performed best on validation data\n",
        "    maxValAccSubsetIndicies = [index for index in range(len(validationAccuracyList)) if validationAccuracyList[index] == max(validationAccuracyList)]\n",
        "    maxValIndividuals = [individualList[index] for index in maxValAccSubsetIndicies]\n",
        "    maxValSubsets = [[list(allFeatures)[index] for index in range(len(individual)) if individual[index] == 1] for individual in maxValIndividuals]\n",
        "\n",
        "\n",
        "    print('\\n---Optimal Feature Subset(s)---\\n')\n",
        "    for index in range(len(maxValAccSubsetIndicies)):\n",
        "        print('Validation Accuracy: \\t\\t' + str(validationAccuracyList[maxValAccSubsetIndicies[index]]))\n",
        "        print('Individual: \\t' + str(maxValIndividuals[index]))\n",
        "        print('Number Features In Subset: \\t' + str(len(maxValSubsets[index])))\n",
        "        print('Feature Subset: ' + str(maxValSubsets[index]))\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Time taken: \",end - start)"
      ],
      "metadata": {
        "id": "xWdXYQ-6vX0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}